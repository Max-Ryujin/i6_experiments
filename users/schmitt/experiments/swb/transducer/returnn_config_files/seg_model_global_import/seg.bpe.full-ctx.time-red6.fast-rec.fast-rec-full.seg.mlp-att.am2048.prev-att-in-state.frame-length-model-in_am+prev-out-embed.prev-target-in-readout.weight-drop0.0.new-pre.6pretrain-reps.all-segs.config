#!rnn.py


from returnn.tf.util.data import Dim
import os
import numpy as np
from subprocess import check_output, CalledProcessError


def _mask(x, batch_axis, axis, pos, max_amount, mask_value=0.0):
    """
    :param tf.Tensor x: (batch,time,feature)
    :param int batch_axis:
    :param int axis:
    :param tf.Tensor pos: (batch,)
    :param int|tf.Tensor max_amount: inclusive
    :param float|int mask_value:
    """
    import tensorflow as tf

    ndim = x.get_shape().ndims
    n_batch = tf.shape(x)[batch_axis]
    dim = tf.shape(x)[axis]
    amount = tf.random.uniform(
        shape=(n_batch,), minval=1, maxval=max_amount + 1, dtype=tf.int32
    )
    pos2 = tf.minimum(pos + amount, dim)
    idxs = tf.expand_dims(tf.range(0, dim), 0)  # (1,dim)
    pos_bc = tf.expand_dims(pos, 1)  # (batch,1)
    pos2_bc = tf.expand_dims(pos2, 1)  # (batch,1)
    cond = tf.logical_and(
        tf.greater_equal(idxs, pos_bc), tf.less(idxs, pos2_bc)
    )  # (batch,dim)
    if batch_axis > axis:
        cond = tf.transpose(cond)  # (dim,batch)
    cond = tf.reshape(
        cond, [tf.shape(x)[i] if i in (batch_axis, axis) else 1 for i in range(ndim)]
    )
    from returnn.tf.util.basic import where_bc

    x = where_bc(cond, mask_value, x)
    return x


def random_mask(x, batch_axis, axis, min_num, max_num, max_dims, mask_value=0.0):
    """
    :param tf.Tensor x: (batch,time,feature)
    :param int batch_axis:
    :param int axis:
    :param int|tf.Tensor min_num:
    :param int|tf.Tensor max_num: inclusive
    :param int|tf.Tensor max_dims: inclusive
    :param float|int mask_value:
    """
    import tensorflow as tf
    from returnn.tf.util.basic import expand_multiple_dims

    n_batch = tf.shape(x)[batch_axis]
    if isinstance(min_num, int) and isinstance(max_num, int) and min_num == max_num:
        num = min_num
    else:
        num = tf.random.uniform(
            shape=(n_batch,), minval=min_num, maxval=max_num + 1, dtype=tf.int32
        )
    # https://github.com/tensorflow/tensorflow/issues/9260
    # https://timvieira.github.io/blog/post/2014/08/01/gumbel-max-trick-and-weighted-reservoir-sampling/
    z = -tf.math.log(
        -tf.math.log(tf.random.uniform((n_batch, tf.shape(x)[axis]), 0, 1))
    )
    _, indices = tf.nn.top_k(z, num if isinstance(num, int) else tf.reduce_max(num))
    # indices should be sorted, and of shape (batch,num), entries (int32) in [0,dim)
    # indices = tf.Print(indices, ["indices", indices, tf.shape(indices)])
    if isinstance(num, int):
        for i in range(num):
            x = _mask(
                x,
                batch_axis=batch_axis,
                axis=axis,
                pos=indices[:, i],
                max_amount=max_dims,
                mask_value=mask_value,
            )
    else:
        _, x = tf.while_loop(
            cond=lambda i, _: tf.less(i, tf.reduce_max(num)),
            body=lambda i, x: (
                i + 1,
                tf.where(
                    expand_multiple_dims(
                        tf.less(i, num), axes=[-1] * (len(x.shape) - len(num.shape))
                    ),
                    _mask(
                        x,
                        batch_axis=batch_axis,
                        axis=axis,
                        pos=indices[:, i],
                        max_amount=max_dims,
                        mask_value=mask_value,
                    ),
                    x,
                ),
            ),
            loop_vars=(0, x),
        )
    return x


def transform(data, network, time_factor=1):
    x = data.placeholder
    import tensorflow as tf

    # summary("features", x)
    step = network.global_train_step
    step1 = tf.where(tf.greater_equal(step, 1000), 1, 0)
    step2 = tf.where(tf.greater_equal(step, 2000), 1, 0)

    def get_masked():
        x_masked = x
        x_masked = random_mask(
            x_masked,
            batch_axis=data.batch_dim_axis,
            axis=data.time_dim_axis,
            min_num=step1 + step2,
            max_num=tf.minimum(
                tf.maximum(tf.shape(x)[data.time_dim_axis] // 100, 2)
                * (1 + step1 + step2 * 2),
                tf.shape(x)[data.time_dim_axis],
            ),
            max_dims=20 // time_factor,
        )
        x_masked = random_mask(
            x_masked,
            batch_axis=data.batch_dim_axis,
            axis=data.feature_dim_axis,
            min_num=step1 + step2,
            max_num=2 + step1 + step2 * 2,
            max_dims=data.dim // 5,
        )
        # summary("features_mask", x_masked)
        return x_masked

    x = network.cond_on_train(get_masked, lambda: x)
    return x


def get_vocab_tf():
    from returnn.datasets.generating import Vocabulary
    from returnn.tf.util.basic import get_shared_vocab

    vocab = Vocabulary.create_vocab(**eval("vocab"))
    labels = vocab.labels  # bpe labels ("@@" at end, or not), excluding blank
    labels = [(l + " ").replace("@@ ", "") for l in labels] + [""]
    labels_t = get_shared_vocab(labels)
    return labels_t


def get_vocab_sym(i):
    """
    :param tf.Tensor i: e.g. [B], int32
    :return: same shape as input, string
    :rtype: tf.Tensor
    """
    import tensorflow as tf

    return tf.gather(params=get_vocab_tf(), indices=i)


def out_str(source, **kwargs):
    # ["prev:out_str", "output_emit", "output"]
    import tensorflow as tf
    from returnn.tf.util.basic import where_bc

    with tf.device("/cpu:0"):
        return source(0) + where_bc(
            source(1), get_vocab_sym(source(2)), tf.constant("")
        )


def targetb_recomb_recog(
    layer, batch_dim, scores_in, scores_base, base_beam_in, end_flags, **kwargs
):
    """
    :param ChoiceLayer layer:
    :param tf.Tensor batch_dim: scalar
    :param tf.Tensor scores_base: (batch,base_beam_in,1). existing beam scores
    :param tf.Tensor scores_in: (batch,base_beam_in,dim). log prob frame distribution
    :param tf.Tensor end_flags: (batch,base_beam_in)
    :param tf.Tensor base_beam_in: int32 scalar, 1 or prev beam size
    :rtype: tf.Tensor
    :return: (batch,base_beam_in,dim), combined scores
    """
    import tensorflow as tf

    prev_str = layer.explicit_search_sources[0].output  # [B*beam], str
    prev_str_t = tf.reshape(prev_str.placeholder, (batch_dim, -1))[:, :base_beam_in]

    # Pre-filter approx (should be much faster), sum approx (better).
    scores_base = tf.reshape(
        get_filtered_score_cpp(
            prev_str_t, tf.reshape(scores_base, (batch_dim, base_beam_in))
        ),
        (batch_dim, base_beam_in, 1),
    )

    scores = scores_in + scores_base  # (batch,beam,dim)

    return scores


def get_filtered_score_op(verbose=False):
    cpp_code = """
  #include "tensorflow/core/framework/op.h"
  #include "tensorflow/core/framework/op_kernel.h"
  #include "tensorflow/core/framework/shape_inference.h"
  #include "tensorflow/core/framework/resource_mgr.h"
  #include "tensorflow/core/framework/resource_op_kernel.h"
  #include "tensorflow/core/framework/tensor.h"
  #include "tensorflow/core/platform/macros.h"
  #include "tensorflow/core/platform/mutex.h"
  #include "tensorflow/core/platform/types.h"
  #include "tensorflow/core/public/version.h"
  #include <cmath>
  #include <map>
  #include <set>
  #include <string>
  #include <tuple>

  using namespace tensorflow;

  REGISTER_OP("GetFilteredScore")
  .Input("prev_str: string")
  .Input("scores: float32")
  //.Input("labels: string")
  .Output("new_scores: float32")
  .SetShapeFn([](::tensorflow::shape_inference::InferenceContext* c) {
      c->set_output(0, c->input(1));
      return Status::OK();
  });

  class GetFilteredScoreOp : public OpKernel {
  public:
  using OpKernel::OpKernel;
  void Compute(OpKernelContext* context) override {
      const Tensor* prev_str = &context->input(0);
      const Tensor* scores = &context->input(1);
      //const Tensor* labels = &context->input(2);

      int n_batch = prev_str->shape().dim_size(0);
      int n_beam = prev_str->shape().dim_size(1);

      Tensor* ret;
      OP_REQUIRES_OK(context, context->allocate_output(0, TensorShape({n_batch, n_beam}), &ret));
      for(int bat = 0; bat < n_batch; ++bat)
          for(int hyp = 0; hyp < n_beam; ++hyp)
              ret->tensor<float, 2>()(bat, hyp) = scores->tensor<float, 2>()(bat, hyp);

      for(int bat = 0; bat < n_batch; ++bat) {
          std::map<tstring, std::set<int> > new_hyps;  // seq -> set of hyp idx

          for(int hyp = 0; hyp < n_beam; ++hyp) {
              auto& seq_set = new_hyps[prev_str->tensor<tstring, 2>()(bat, hyp)];
              seq_set.insert(hyp);
          }

          for(const auto& items : new_hyps) {
              if(std::get<1>(items).size() > 1) {
                  float best_score = 0.;
                  int best_idx = -1;
                  for(int idx : std::get<1>(items)) {
                      float score = scores->tensor<float, 2>()(bat, idx);
                      if(score > best_score || best_idx == -1) {
                          best_score = score;
                          best_idx = idx;
                      }
                  }

                  float sum_score = 0.;
                  for(int idx : std::get<1>(items)) {
                      float score = scores->tensor<float, 2>()(bat, idx);
                      sum_score += expf(score - best_score);
                  }
                  sum_score = logf(sum_score) + best_score;

                  for(int idx : std::get<1>(items)) {
                      if(idx != best_idx)
                          ret->tensor<float, 2>()(bat, idx) = -std::numeric_limits<float>::infinity();
                      else
                          ret->tensor<float, 2>()(bat, idx) = sum_score;
                  }
              }
          }
      }
  }
  };
  REGISTER_KERNEL_BUILDER(Name("GetFilteredScore").Device(DEVICE_CPU), GetFilteredScoreOp);
  """
    from returnn.tf.util.basic import OpCodeCompiler

    compiler = OpCodeCompiler(
        base_name="GetFilteredScore",
        code_version=1,
        code=cpp_code,
        is_cpp=True,
        use_cuda_if_available=False,
        verbose=verbose,
    )
    tf_mod = compiler.load_tf_module()
    return tf_mod.get_filtered_score


def get_filtered_score_cpp(prev_str, scores):
    """
    :param tf.Tensor prev_str: (batch,beam)
    :param tf.Tensor scores: (batch,beam)
    :param list[bytes] labels: len (dim)
    :return: scores with logsumexp at best, others -inf, (batch,beam)
    :rtype: tf.Tensor
    """
    import tensorflow as tf

    with tf.device("cpu:0"):
        return get_filtered_score_op()(prev_str, scores)


def custom_construction_algo(idx, net_dict):
    if idx > 30:
        return None
    net_dict["#config"] = {}
    if idx is not None:
        # learning rate warm up
        lr_warmup = list(
            np.linspace(
                net_dict["#info"]["learning_rate"] * 0.1,
                net_dict["#info"]["learning_rate"],
                num=10,
            )
        )
        if idx < len(lr_warmup):
            net_dict["#config"]["learning_rate"] = lr_warmup[idx]

    # encoder construction
    start_num_lstm_layers = 2
    final_num_lstm_layers = 6
    num_lstm_layers = final_num_lstm_layers
    if idx is not None:
        idx = max(idx, 0) // 6  # each index is used 6 times
        num_lstm_layers = (
            idx + start_num_lstm_layers
        )  # 2, 3, 4, 5, 6 (each for 6 epochs)
        idx = num_lstm_layers - final_num_lstm_layers
        num_lstm_layers = min(num_lstm_layers, final_num_lstm_layers)

    if final_num_lstm_layers > start_num_lstm_layers:
        start_dim_factor = 0.5
        # grow_frac values: 0, 1/4, 1/2, 3/4, 1
        grow_frac = 1.0 - float(final_num_lstm_layers - num_lstm_layers) / (
            final_num_lstm_layers - start_num_lstm_layers
        )
        # dim_frac values: 0.5, 5/8, 3/4, 7/8, 1
        dim_frac = start_dim_factor + (1.0 - start_dim_factor) * grow_frac
    else:
        dim_frac = 1.0
    net_dict["#info"].update(
        {"dim_frac": dim_frac, "num_lstm_layers": num_lstm_layers, "pretrain_idx": idx}
    )

    time_reduction = (
        net_dict["#info"]["time_red"]
        if num_lstm_layers >= 3
        else [int(np.prod(net_dict["#info"]["time_red"]))]
    )

    # Add encoder BLSTM stack
    src = "conv_merged"
    lstm_dim = net_dict["#info"]["lstm_dim"]
    l2 = net_dict["#info"]["l2"]
    if num_lstm_layers >= 1:
        net_dict.update(
            {
                "lstm0_fw": {
                    "class": "rec",
                    "unit": "nativelstm2",
                    "n_out": int(lstm_dim * dim_frac),
                    "L2": l2,
                    "direction": 1,
                    "from": src,
                    "trainable": True,
                },
                "lstm0_bw": {
                    "class": "rec",
                    "unit": "nativelstm2",
                    "n_out": int(lstm_dim * dim_frac),
                    "L2": l2,
                    "direction": -1,
                    "from": src,
                    "trainable": True,
                },
            }
        )
        src = ["lstm0_fw", "lstm0_bw"]
    for i in range(1, num_lstm_layers):
        red = time_reduction[i - 1] if (i - 1) < len(time_reduction) else 1
        net_dict.update(
            {
                "lstm%i_pool"
                % (i - 1): {
                    "class": "pool",
                    "mode": "max",
                    "padding": "same",
                    "pool_size": (red,),
                    "from": src,
                }
            }
        )
        src = "lstm%i_pool" % (i - 1)
        net_dict.update(
            {
                "lstm%i_fw"
                % i: {
                    "class": "rec",
                    "unit": "nativelstm2",
                    "n_out": int(lstm_dim * dim_frac),
                    "L2": l2,
                    "direction": 1,
                    "from": src,
                    "dropout": 0.3 * dim_frac,
                    "trainable": True,
                },
                "lstm%i_bw"
                % i: {
                    "class": "rec",
                    "unit": "nativelstm2",
                    "n_out": int(lstm_dim * dim_frac),
                    "L2": l2,
                    "direction": -1,
                    "from": src,
                    "dropout": 0.3 * dim_frac,
                    "trainable": True,
                },
            }
        )
        src = ["lstm%i_fw" % i, "lstm%i_bw" % i]
    net_dict["encoder0"] = {"class": "copy", "from": src}  # dim: EncValueTotalDim

    # if necessary, include dim_frac in the attention values
    # TODO check if this is working
    try:
        if net_dict["output"]["unit"]["att_val"]["class"] == "linear":
            net_dict["output"]["unit"]["att_val"]["n_out"] = 2 * int(
                lstm_dim * dim_frac
            )
    except KeyError:
        pass

    if "task" not in net_dict["#info"]:
        net_dict["label_model"]["unit"]["label_prob"]["loss_opts"][
            "label_smoothing"
        ] = 0

    return net_dict


_alignment = None
_cf_cache = {}
accum_grad_multiple_step = 2
adam = True
batch_size = 4000
batching = "random"
beam_size = 12
chunking = ({"alignment": 60, "data": 360}, {"alignment": 30, "data": 180})
debug_print_layer_output_template = True
device = "gpu"
epoch_split = 6
extern_data = {
    "alignment": {
        "dim": 1031,
        "same_dim_tags_as": {
            "t": Dim(kind=Dim.Types.Spatial, description="output-len")
        },
        "sparse": True,
    },
    "bpe": {"dim": 1030, "sparse": True},
    "data": {
        "dim": 40,
        "same_dim_tags_as": {"t": Dim(kind=Dim.Types.Spatial, description="time")},
    },
    "targetb": {"available_for_inference": False, "dim": 1031, "sparse": True},
}
gradient_clip = 0
gradient_noise = 0.0
learning_rate = 0.001
learning_rate_control = "newbob_multi_epoch"
learning_rate_control_error_measure = "dev_error_output/label_prob"
learning_rate_control_min_num_epochs_per_new_lr = 3
learning_rate_control_relative_error_relative_lr = True
load = "/u/schmitt/experiments/transducer/alias/glob.best-model.bpe.time-red6.am2048.6pretrain-reps.no-weight-feedback.no-l2.ctx-use-bias.all-segs/train/output/models/epoch.150"
log = ["./returnn.log"]
log_batch_size = True
log_verbosity = 3
max_seq_length = 0
max_seqs = 200
min_learning_rate = 2e-05
need_data = False
network = {
    "#info": {
        "l2": 0.0001,
        "learning_rate": 0.001,
        "lstm_dim": 1024,
        "task": "search",
        "time_red": [3, 2],
    },
    "conv0": {
        "activation": None,
        "auto_use_channel_first": False,
        "class": "conv",
        "filter_size": (3, 3),
        "from": "source0",
        "n_out": 32,
        "padding": "same",
        "with_bias": True,
    },
    "conv0p": {
        "class": "pool",
        "from": "conv0",
        "mode": "max",
        "padding": "same",
        "pool_size": (1, 2),
        "use_channel_first": False,
    },
    "conv1": {
        "activation": None,
        "auto_use_channel_first": False,
        "class": "conv",
        "filter_size": (3, 3),
        "from": "conv0p",
        "n_out": 32,
        "padding": "same",
        "with_bias": True,
    },
    "conv1p": {
        "class": "pool",
        "from": "conv1",
        "mode": "max",
        "padding": "same",
        "pool_size": (1, 2),
        "use_channel_first": False,
    },
    "conv_merged": {"axes": "static", "class": "merge_dims", "from": "conv1p"},
    "decision": {
        "class": "decide",
        "from": "output_wo_b",
        "loss": "edit_distance",
        "only_on_search": True,
        "target": "bpe",
    },
    "encoder": {"class": "copy", "from": "encoder0"},
    "ctc_out": {
        "class": "softmax",
        "from": "encoder",
        "n_out": 1031,
        "with_bias": False,
    },
    "encoder0": {"class": "copy", "from": ["lstm5_fw", "lstm5_bw"]},
    "lstm0_bw": {
        "L2": 0.0001,
        "class": "rec",
        "direction": -1,
        "from": "conv_merged",
        "n_out": 1024,
        "trainable": True,
        "unit": "nativelstm2",
    },
    "lstm0_fw": {
        "L2": 0.0001,
        "class": "rec",
        "direction": 1,
        "from": "conv_merged",
        "n_out": 1024,
        "trainable": True,
        "unit": "nativelstm2",
    },
    "lstm0_pool": {
        "class": "pool",
        "from": ["lstm0_fw", "lstm0_bw"],
        "mode": "max",
        "padding": "same",
        "pool_size": (3,),
    },
    "lstm1_bw": {
        "L2": 0.0001,
        "class": "rec",
        "direction": -1,
        "dropout": 0.3,
        "from": "lstm0_pool",
        "n_out": 1024,
        "trainable": True,
        "unit": "nativelstm2",
    },
    "lstm1_fw": {
        "L2": 0.0001,
        "class": "rec",
        "direction": 1,
        "dropout": 0.3,
        "from": "lstm0_pool",
        "n_out": 1024,
        "trainable": True,
        "unit": "nativelstm2",
    },
    "lstm1_pool": {
        "class": "pool",
        "from": ["lstm1_fw", "lstm1_bw"],
        "mode": "max",
        "padding": "same",
        "pool_size": (2,),
    },
    "lstm2_bw": {
        "L2": 0.0001,
        "class": "rec",
        "direction": -1,
        "dropout": 0.3,
        "from": "lstm1_pool",
        "n_out": 1024,
        "trainable": True,
        "unit": "nativelstm2",
    },
    "lstm2_fw": {
        "L2": 0.0001,
        "class": "rec",
        "direction": 1,
        "dropout": 0.3,
        "from": "lstm1_pool",
        "n_out": 1024,
        "trainable": True,
        "unit": "nativelstm2",
    },
    "lstm2_pool": {
        "class": "pool",
        "from": ["lstm2_fw", "lstm2_bw"],
        "mode": "max",
        "padding": "same",
        "pool_size": (1,),
    },
    "lstm3_bw": {
        "L2": 0.0001,
        "class": "rec",
        "direction": -1,
        "dropout": 0.3,
        "from": "lstm2_pool",
        "n_out": 1024,
        "trainable": True,
        "unit": "nativelstm2",
    },
    "lstm3_fw": {
        "L2": 0.0001,
        "class": "rec",
        "direction": 1,
        "dropout": 0.3,
        "from": "lstm2_pool",
        "n_out": 1024,
        "trainable": True,
        "unit": "nativelstm2",
    },
    "lstm3_pool": {
        "class": "pool",
        "from": ["lstm3_fw", "lstm3_bw"],
        "mode": "max",
        "padding": "same",
        "pool_size": (1,),
    },
    "lstm4_bw": {
        "L2": 0.0001,
        "class": "rec",
        "direction": -1,
        "dropout": 0.3,
        "from": "lstm3_pool",
        "n_out": 1024,
        "trainable": True,
        "unit": "nativelstm2",
    },
    "lstm4_fw": {
        "L2": 0.0001,
        "class": "rec",
        "direction": 1,
        "dropout": 0.3,
        "from": "lstm3_pool",
        "n_out": 1024,
        "trainable": True,
        "unit": "nativelstm2",
    },
    "lstm4_pool": {
        "class": "pool",
        "from": ["lstm4_fw", "lstm4_bw"],
        "mode": "max",
        "padding": "same",
        "pool_size": (1,),
    },
    "lstm5_bw": {
        "L2": 0.0001,
        "class": "rec",
        "direction": -1,
        "dropout": 0.3,
        "from": "lstm4_pool",
        "n_out": 1024,
        "trainable": True,
        "unit": "nativelstm2",
    },
    "lstm5_fw": {
        "L2": 0.0001,
        "class": "rec",
        "direction": 1,
        "dropout": 0.3,
        "from": "lstm4_pool",
        "n_out": 1024,
        "trainable": True,
        "unit": "nativelstm2",
    },
    "output": {
        "back_prop": False,
        "class": "rec",
        "from": "encoder",
        "include_eos": True,
        "size_target": None,
        "initial_output": 0,
        "target": "targetb",
        "unit": {
            "am": {"class": "copy", "from": "data:source"},
            "att": {"axes": "except_time", "class": "merge_dims", "from": "att0"},
            "att0": {
                "add_var2_if_empty": False,
                "class": "dot",
                "from": ["att_val_split", "att_weights"],
                "reduce": "stag:att_t",
                "var1": "f",
                "var2": None,
            },
            "att_ctx": {
                "L2": 0.0001,
                "activation": None,
                "class": "linear",
                "dropout": 0.2,
                "from": "segments",
                "n_out": 1024,
                "with_bias": True,
                "name_scope": "/enc_ctx"
            },
            "att_energy": {
                "class": "reinterpret_data",
                "from": "att_energy0",
                "is_output_layer": False,
                "set_dim_tags": {
                    "f": Dim(
                        kind=Dim.Types.Spatial, description="att_heads", dimension=1
                    )
                },
            },
            "att_energy0": {
                "activation": None,
                "name_scope": "energy",
                "class": "linear",
                "from": ["energy_tanh"],
                "n_out": 1,
                "with_bias": False,
            },
            "att_energy_in": {
                "class": "combine",
                "from": ["att_ctx", "att_query"],
                "kind": "add",
                "n_out": 1024,
            },
            "att_query": {
                "activation": None,
                "class": "linear",
                "from": "lm",
                "is_output_layer": False,
                "n_out": 1024,
                "with_bias": False,
            },
            "att_val": {"class": "copy", "from": "segments"},
            "att_val_split": {
                "class": "reinterpret_data",
                "from": "att_val_split0",
                "set_dim_tags": {
                    "dim:1": Dim(
                        kind=Dim.Types.Spatial, description="att_heads", dimension=1
                    )
                },
            },
            "att_val_split0": {
                "axis": "f",
                "class": "split_dims",
                "dims": (1, -1),
                "from": "att_val",
            },
            "att_weights": {
                "class": "dropout",
                "dropout": 0.1,
                "dropout_noise_shape": {"*": None},
                "from": "att_weights0",
                "is_output_layer": False,
            },
            "att_weights0": {
                "axis": "stag:att_t",
                "class": "softmax_over_spatial",
                "energy_factor": 0.03125,
                "from": "att_energy",
            },
            "const1": {"class": "constant", "value": 1},
            "const0.0": {"axis": "F", "class": "expand_dims", "from": "const0.0_0"},
            "const0.0_0": {"class": "constant", "value": 0.0, "with_batch_dim": True},
            "enc_length": {
                "class": "combine",
                "from": ["enc_length0", "const1"],
                "kind": "sub",
            },
            "enc_length0": {"axis": "t", "class": "length", "from": "base:encoder"},
            "is_last_frame": {
                "class": "compare",
                "from": [":i", "enc_length"],
                "kind": "greater_equal",
            },
            "max_seg_len": {"class": "constant", "value": 19},
            "max_seg_len_or_last_frame": {
                "class": "combine",
                "from": ["is_segment_longer_than_max", "is_last_frame"],
                "kind": "logical_or",
            },
            "is_segment_longer_than_max": {
                "class": "compare",
                "from": ["segment_lens", "max_seg_len"],
                "kind": "greater",
            },
            "blank_log_prob": {
                "class": "switch",
                "condition": "max_seg_len_or_last_frame",
                "false_from": "const0.0",
                "true_from": -10000000.0,
            },
            "emit_log_prob": {
                "class": "switch",
                "condition": "max_seg_len_or_last_frame",
                "false_from": "const0.0",
                "true_from": "const0.0",
            },
            "emit_log_prob0": {
                "activation": "log_sigmoid",
                "class": "activation",
                "from": "emit_prob0",
            },
            "emit_log_prob_scaled": {
                "class": "eval",
                "eval": "0.0 * source(0)",
                "from": "emit_log_prob0",
            },
            "energy_tanh": {
                "activation": "tanh",
                "class": "activation",
                "from": ["att_energy_in"],
            },
            "label_log_prob": {
                "class": "combine",
                "from": ["label_log_prob0", "emit_log_prob"],
                "kind": "add",
            },
            "label_log_prob0": {
                "activation": "log_softmax",
                "class": "linear",
                "name_scope": "label_prob",
                "dropout": 0.3,
                "from": "readout",
                "n_out": 1030,
            },
            "lm": {"class": "unmask", "from": "lm_masked", "mask": "prev:output_emit"},
            "lm_masked": {
                "class": "masked_computation",
                "from": "prev_non_blank_embed",
                "name_scope": "",
                "mask": "prev:output_emit",
                "unit": {
                    "class": "subnetwork",
                    "from": "data",
                    "subnetwork": {
                        "input_embed": {
                            "axes": "except_time",
                            "class": "merge_dims",
                            "from": "input_embed0",
                        },
                        "input_embed0": {
                            "class": "window",
                            "from": "data",
                            "window_left": 0,
                            "window_right": 0,
                            "window_size": 1,
                        },
                        "lm": {
                            "class": "rec",
                            "from": ["input_embed", "base:prev:att"],
                            "n_out": 1024,
                            "name_scope": "lm/rec",
                            "unit": "nativelstm2",
                        },
                        "output": {"class": "copy", "from": "lm"},
                    },
                },
            },
            "output": {
                "beam_size": 12,
                "cheating": None,
                "class": "choice",
                "from": "output_log_prob",
                "initial_output": 0,
                "input_type": "log_prob",
                "length_normalization": False,
                "target": "targetb",
            },
            "output_emit": {
                "class": "compare",
                "from": "output",
                "initial_output": True,
                "kind": "not_equal",
                "value": 1030,
            },
            "output_log_prob": {
                "class": "copy",
                "from": ["label_log_prob", "blank_log_prob"],
            },
            "prev_out_non_blank_masked": {
                "class": "masked_computation",
                "from": "prev_out_non_blank",
                "initial_output": 0,
                "name_scope": "",
                "mask": "prev:output_emit",
                "unit": {
                    "class": "copy", "from": "data", "initial_output": 0,},
            },
            "prev_non_blank_embed": {
                "activation": None,
                "name_scope": "target_embed",
                "class": "linear",
                "from": "prev_out_non_blank_masked",
                "n_out": 621,
                "with_bias": False,
            },
            "prev_out_embed": {
                "activation": None,
                "class": "linear",
                "from": "prev:output",
                "n_out": 128,
            },
            "prev_out_non_blank": {
                "class": "reinterpret_data",
                "from": "prev:output",
                "set_sparse": True,
                "set_sparse_dim": 1030,
                "initial_output": 0,
            },
            "readout": {
                "class": "reduce_out",
                "from": "readout_in",
                "mode": "max",
                "num_pieces": 2,
            },
            "readout_in": {
                "activation": None,
                "class": "linear",
                "from": ["lm", "prev_non_blank_embed", "att"],
                "n_out": 1000,
            },
            "segment_lens": {
                "class": "combine",
                "from": ["segment_lens0", "const1"],
                "is_output_layer": True,
                "kind": "add",
            },
            "segment_lens0": {
                "class": "combine",
                "from": [":i", "segment_starts"],
                "kind": "sub",
            },
            "segment_starts": {
                "class": "switch",
                "condition": "prev:output_emit",
                "false_from": "prev:segment_starts",
                "initial_output": 0,
                "is_output_layer": True,
                "true_from": ":i",
            },
            "segments": {
                "class": "reinterpret_data",
                "from": "segments0",
                "set_dim_tags": {
                    "stag:sliced-time:segments": Dim(
                        kind=Dim.Types.Spatial, description="att_t"
                    )
                },
            },
            "segments0": {
                "class": "slice_nd",
                "from": "base:encoder",
                "size": "segment_lens",
                "start": "segment_starts",
            },
        },
    },
    "output_non_blank": {
        "class": "compare",
        "from": "output",
        "kind": "not_equal",
        "value": 1030,
    },
    "output_non_sil": {
        "class": "compare",
        "from": "output",
        "kind": "not_equal",
        "value": -1,
    },
    "output_non_sil_non_blank": {
        "class": "combine",
        "from": ["output_non_sil", "output_non_blank"],
        "is_output_layer": True,
        "kind": "logical_and",
    },
    "output_wo_b": {
        "class": "reinterpret_data",
        "from": "output_wo_b0",
        "set_sparse_dim": 1030,
    },
    "output_wo_b0": {
        "class": "masked_computation",
        "from": "output",
        "mask": "output_non_sil_non_blank",
        "unit": {"class": "copy"},
    },
    "source": {
        "class": "eval",
        "eval": "self.network.get_config().typed_value('transform')(source(0, as_data=True), network=self.network)",
    },
    "source0": {"axis": "F", "class": "split_dims", "dims": (-1, 1), "from": "source"},
}
newbob_learning_rate_decay = 0.7
newbob_multi_num_epochs = 6
newbob_multi_update_interval = 1
num_epochs = 50
optimizer_epsilon = 1e-08
pretrain = {"construction_algo": custom_construction_algo, "copy_param_mode": "subset"}
rasr_config = "/u/schmitt/experiments/transducer/config/rasr-configs/merged.config"
search_data = {
    "bpe": {
        "bpe_file": "/work/asr3/irie/data/switchboard/subword_clean/ready/swbd_clean.bpe_code_1k",
        "seq_postfix": [0],
        "vocab_file": "/work/asr3/irie/data/switchboard/subword_clean/ready/vocab.swbd_clean.bpe_code_1k",
    },
    "class": "ExternSprintDataset",
    "input_stddev": 3.0,
    "sprintConfigStr": [
        "--config=/u/schmitt/experiments/transducer/work/i6_core/rasr/config/WriteRasrConfigJob.eZtBbOFe0zu9/output/rasr.config",
        "--*.corpus.segment-order-shuffle=true",
        "--*.corpus.segments.file=/work/asr3/zeyer/schmitt/tests/swb1/bpe-transducer_decoding-test/segments.1",
        "--*.segment-order-sort-by-time-length=true",
        "--*.segment-order-sort-by-time-length-chunk-size=-1",
    ],
    "sprintTrainerExecPath": "/u/schmitt/src/rasr/arch/linux-x86_64-standard/nn-trainer.linux-x86_64-standard",
    "suppress_load_seqs_print": True,
}
search_do_eval = 0
search_output_file = "/u/schmitt/experiments/transducer/work/i6_core/returnn/search/ReturnnSearchJob.uoQneFTBBBe9/output/search_out"
search_output_file_format = "py"
search_output_layer = "decision"
stop_on_nonfinite_train_score = False
target = "bpe"
target_num_labels = 1030
targetb_blank_idx = 1030
targetb_num_labels = 1031
task = "search"
tf_log_memory_usage = True
truncation = -1
use_learning_rate_control_always = True
use_tensorflow = True
vocab = {
    "bpe_file": "/work/asr3/irie/data/switchboard/subword_clean/ready/swbd_clean.bpe_code_1k",
    "seq_postfix": [0],
    "vocab_file": "/work/asr3/irie/data/switchboard/subword_clean/ready/vocab.swbd_clean.bpe_code_1k",
}

config = {}

locals().update(**config)
