# DEPRICATED !!! SE config_baseline_04_big_short.py
# This is 'big-short'
# Train for only 3 full epochs i.e.: 120 sub epochs 40 split
# Model as about 86 M params

from atexit import register
from typing import OrderedDict
from recipe.i6_experiments.users.schupp.hybrid_hmm_nn.args import setup_god as god
from recipe.i6_experiments.users.schupp.hybrid_hmm_nn.args import conformer_config_returnn_baseargs as experiment_config_args
from recipe.i6_experiments.users.schupp.hybrid_hmm_nn.args.conformer_args_003_bigger_baseline import original_args_big_baseline_00
from recipe.i6_experiments.users.schupp.hybrid_hmm_nn.args import conformer_returnn_dict_network_generator

from sisyphus import gs
import copy
import numpy
import math

import inspect

OUTPUT_PATH = "conformer/baseline_03_big/"
gs.ALIAS_AND_OUTPUT_SUBDIR = OUTPUT_PATH

def get_defaults():
  args = copy.deepcopy(original_args_big_baseline_00)
  return args

def make_experiment_03_rqmt(
  args, 
  NAME,
  aux_loss_layers = [6],
  test_construct = False
  ):
  experiment_data = god.create_experiment_world_003( 
    name=NAME,
    output_path=OUTPUT_PATH,
    config_base_args=args.config_args,
    conformer_create_func=conformer_returnn_dict_network_generator.make_conformer_03_feature_stacking_auxilary_loss,
    conformer_func_args=OrderedDict(
      # sampling args
      sampling_func_args = args.sampling_default_args,

      # Feed forward args, both the same by default
      ff1_func_args = args.ff_default_args,
      ff2_func_args = args.ff_default_args,

      # Self attention args
      sa_func_args = args.sa_default_args,

      # Conv mod args
      conv_func_args = args.conv_default_args,

      # Shared model args
      shared_model_args = args.shared_network_args,

      auxilary_at_layer = aux_loss_layers,
      auxilary_loss_args = args.auxilary_loss_args,

      # Conformer args
      **args.conformer_defaults ),
      returnn_train_post_config=args.returnn_train_post_config,
      returnn_rasr_args_defaults=args.returnn_rasr_args_defaults,

      extra_recog_epochs=[5], # Basicly early test epoch

      test_construction=test_construct,
  )

def baseline_big_short():
  args = get_defaults()
  NAME = "baseline_03_bigh_short"

  make_experiment_03_rqmt(args, NAME)

def no_aux_loss():
  args = get_defaults()
  NAME = "baseline_03_bigh_short+no-aux-loss"

  make_experiment_03_rqmt(args, NAME, aux_loss_layers=[])

def batchnorm_no_ln(): # TODO
  args = get_defaults()
  NAME = "baseline_03_bigh_short+batch-norm"

def main():
  baseline_big_short()

  no_aux_loss()